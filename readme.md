# Cotin IA - Sistema RAG para TransparÃªncia em Compras PÃºblicas ğŸš€ğŸ“Š

## ğŸ“‘ VisÃ£o Geral

O **Cotin IA** Ã© um sistema avanÃ§ado de RecuperaÃ§Ã£o Aumentada por GeraÃ§Ã£o (RAG) desenvolvido pela **CoordenaÃ§Ã£o de TransparÃªncia e InformaÃ§Ãµes GerenciaisÂ â€” COTIN/CGGES/DELOG**. Especializado em dados abertos sobre compras pÃºblicas, o assistente fornece respostas precisas e objetivas, fundamentadas nos normativos do **Compras.Gov**, **Lei de Acesso Ã  InformaÃ§Ã£o (LAIÂ â€” LeiÂ nÂºÂ 12.527/2011)** e outras regulamentaÃ§Ãµes pertinentes.

---

## ğŸŒŸ Funcionalidades Principais

* **Processamento multiâ€‘formato**: importa e processa arquivos **PDF, DOCX, XLSX, CSV e bancos de dados SQLite**.
* **Processamento avanÃ§ado de texto**: normaliza e divide textos em *chunks* otimizados para indexaÃ§Ã£o e recuperaÃ§Ã£o eficiente.
* **Busca semÃ¢ntica inteligente**: utiliza *embeddings* vetoriais e similaridade de cosseno.
* **Filtragem por relevÃ¢ncia**: sistema de pontuaÃ§Ã£o que combina similaridade semÃ¢ntica e presenÃ§a de palavrasâ€‘chave.
* **Respostas contextuais**: gera respostas baseadas nos documentos recuperados e no histÃ³rico de conversa.
* **MemÃ³ria de conversaÃ§Ã£o**: mantÃ©m o histÃ³rico recente para maior contextualizaÃ§Ã£o.
* **Interface interativa**: integraÃ§Ã£o com Chainlit para uma experiÃªncia de chat intuitiva.

---

## ğŸ—ï¸ Arquitetura Modular

```text
project/
â”œâ”€â”€ app.py               # Ponto de entrada principal
â”œâ”€â”€ requirements.txt     # DependÃªncias do projeto
â”œâ”€â”€ .env                 # VariÃ¡veis de ambiente
â”œâ”€â”€ chainlit.md          # DocumentaÃ§Ã£o da interface
â”œâ”€â”€ docs/                # Documentos para processamento
â”œâ”€â”€ chroma_db/           # Banco de dados vetorial
â””â”€â”€ src/                 # CÃ³digoâ€‘fonte organizado
    â”œâ”€â”€ loaders/         # Carregadores de documentos
    â”œâ”€â”€ processing/      # Processamento de texto
    â”œâ”€â”€ retrieval/       # RecuperaÃ§Ã£o de documentos
    â”œâ”€â”€ llm/             # InteraÃ§Ã£o com LLM
    â”œâ”€â”€ config/          # ConfiguraÃ§Ãµes
    â””â”€â”€ ui/              # Interface com usuario
```

---

## ğŸ› ï¸ Tecnologias Utilizadas

* **Python 3.9+**
* **LangChain**
* **DatabricksÂ Embeddings**
* **ChatDatabricks**
* **ChromaDB**
* **Chainlit**
* **scikitâ€‘learn** (similaridade de cosseno)
* **PyMuPDF** & **Docx2txt**
* **Pandas** & **Openpyxl**
* **SQLite**

---

## ğŸ“‹ PrÃ©â€‘requisitos

1. Python **3.9** ou superior
2. Acesso Ã  plataforma **Databricks** (para embeddings e LLM)
3. Pelo menos **4â€¯GB** de RAM disponÃ­vel
4. EspaÃ§o em disco suficiente para armazenar documentos e o banco vetorial

---

## ğŸš€ Guia de InstalaÃ§Ã£o e ExecuÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/gestaogovbr/chatbot-cotin.git
cd chatbot-cotin.git
```

### 2. Crie e ative um ambiente virtual

**Windows**

```bash
python -m venv venv
venv\Scripts\activate
```

**Linux / macOS**

```bash
python -m venv venv
source venv/bin/activate
```

### 3. Instale as dependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Configure as variÃ¡veis de ambiente

Crie o arquivo **.env** na raiz do projeto:

```env
DATABRICKS_HOST=seu-host-databricks
DATABRICKS_TOKEN=seu-token-databricks
```

### 5. Adicione documentos Ã  base de conhecimento

```bash
mkdir -p docs
# copie seus PDF, DOCX, XLSX e arquivos .db para a pasta docs/
```

### 6. Execute a aplicaÃ§Ã£o

```bash
chainlit run app.py
```

Acesse **[http://localhost:8000](http://localhost:8000)** e comece a fazer perguntas!

---

## ğŸ”„ Utilizando Modelos Alternativos

O Cotin IA Ã© flexÃ­vel e suporta diversos provedores de *embeddings* e LLMs.

### OpenAI

```python
# src/config/settings.py
config = {
    # ...
    "openai_api_key": os.getenv("OPENAI_API_KEY"),
}

# src/retrieval/embeddings.py
from langchain_openai import OpenAIEmbeddings

def get_embeddings(config):
    return OpenAIEmbeddings(
        api_key=config["openai_api_key"],
        model="text-embedding-3-small",
    )

# src/llm/chain.py
from langchain_openai import ChatOpenAI

def get_llm(config):
    return ChatOpenAI(
        api_key=config["openai_api_key"],
        model_name="gpt-4o",
        temperature=config["temperature"],
        max_tokens=config["max_tokens"],
    )
```

### Modelos locais (Ollama)

```python
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama

def get_embeddings(config):
    return OllamaEmbeddings(model="nomic-embed-text")

def get_llm(config):
    return Ollama(
        model="llama3",
        temperature=config["temperature"],
    )
```

### HuggingFace

```python
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEndpoint

def get_embeddings(config):
    return HuggingFaceEmbeddings(model_name="BAAI/bge-large-en-v1.5")

def get_llm(config):
    return HuggingFaceEndpoint(
        endpoint_url="https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2",
        huggingfacehub_api_token=config["hf_token"],
        max_new_tokens=config["max_tokens"],
        temperature=config["temperature"],
    )
```

---

## ğŸ¤ ContribuiÃ§Ãµes

1. FaÃ§a um **fork** do projeto.

2. Crie uma *branch* para sua feature:

   ```bash
   git checkout -b feature/minha-feature
   ```

3. *Commit* das mudanÃ§as:

   ```bash
   git commit -m "feat: adiciona minha-feature"
   ```

4. *Push* para o seu fork:

   ```bash
   git push origin feature/minha-feature
   ```

5. Abra um **Pull Request**.

---

## ğŸ“œ LicenÃ§a

DistribuÃ­do sob a [licenÃ§aÂ MIT](LICENSE).

---

## ğŸ‘¥ Equipe

**CoordenaÃ§Ã£o de TransparÃªncia e InformaÃ§Ãµes Gerenciais â€” COTIN**

* **Coordenador:** Magnum Costa de Oliveira
* **Equipe:**

  * Guilherme Fonseca De Noronha Rocha
  * Stefano Terci Gasperazzo
  * Jose Maria De Melo Junior
  * Luiz Gonzaga de Oliveira
  * AndrÃ© Ruperto de MacÃªdo
